{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpFWQV5ihUO7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    brier_score_loss, roc_auc_score, average_precision_score,\n",
        "    mean_absolute_error, mean_squared_error, confusion_matrix,\n",
        "    RocCurveDisplay, PrecisionRecallDisplay\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. METRICS & CONFIGURATION (Identical to previous script)\n",
        "# ==========================================\n",
        "\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    binids = np.digitize(y_prob, bins) - 1\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        idx = binids == i\n",
        "        if np.any(idx):\n",
        "            acc_in_bin = y_true[idx].mean()\n",
        "            conf_in_bin = y_prob[idx].mean()\n",
        "            prop_in_bin = idx.mean()\n",
        "            ece += np.abs(acc_in_bin - conf_in_bin) * prop_in_bin\n",
        "    return ece\n",
        "\n",
        "def frost_metrics(df: pd.DataFrame):\n",
        "    rows = []\n",
        "    for h, g in df.groupby(\"horizon_h\", sort=True):\n",
        "        y, p = g[\"y_event\"].values, g[\"p_event\"].values\n",
        "        mask = ~np.isnan(y) & ~np.isnan(p)\n",
        "        y, p = y[mask], p[mask]\n",
        "        g = g[mask]\n",
        "\n",
        "        if len(y) == 0: continue\n",
        "\n",
        "        brier = brier_score_loss(y, np.clip(p, 0, 1))\n",
        "        ece   = expected_calibration_error(y, p, n_bins=10)\n",
        "\n",
        "        if len(np.unique(y)) < 2:\n",
        "            roc = np.nan\n",
        "            pr   = np.nan\n",
        "        else:\n",
        "            try: roc = roc_auc_score(y, p)\n",
        "            except: roc = np.nan\n",
        "            try: pr  = average_precision_score(y, p)\n",
        "            except: pr  = np.nan\n",
        "\n",
        "        mae  = float(mean_absolute_error(g[\"y_temp\"], g[\"yhat_temp\"]))\n",
        "        rmse = float(np.sqrt(mean_squared_error(g[\"y_temp\"], g[\"yhat_temp\"])))\n",
        "        bias = float((g[\"yhat_temp\"] - g[\"y_temp\"]).mean())\n",
        "\n",
        "        rows.append({\n",
        "            \"horizon_h\": int(h),\n",
        "            \"brier\": brier, \"ece\": ece, \"roc_auc\": roc, \"pr_auc\": pr,\n",
        "            \"mae\": mae, \"rmse\": rmse, \"bias\": bias,\n",
        "            \"n_samples\": int(len(g))\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values(\"horizon_h\")\n",
        "\n",
        "CONFIG = {\n",
        "    'horizon': [3, 6, 12, 24],\n",
        "    'frost_threshold': 0.0,\n",
        "    'train_file_path': '/content/train_set_filled_w_Mean_cleaned.csv',\n",
        "    'test_file_path': '/content/test_set_filled_w_Mean_cleaned.csv',\n",
        "    'CONFUSION_THRESHOLD': 0.5 # Threshold for converting probabilities to classes\n",
        "}\n",
        "\n",
        "# Tuned Hyperparameters (As determined in your previous step)\n",
        "TUNED_PARAMS = {\n",
        "    3: {\n",
        "        'clf': {'colsample_bytree': float(0.9932923543227152), 'gamma': float(2.3338144662398994), 'learning_rate': float(0.18198808134726413), 'max_depth': 9, 'n_estimators': 70, 'subsample': float(0.7801997007878172)},\n",
        "        'reg': {'alpha': float(0.05808361216819946), 'colsample_bytree': float(0.9464704583099741), 'lambda': float(0.6011150117432088), 'learning_rate': float(0.1516145155592091), 'max_depth': 8, 'n_estimators': 102, 'subsample': float(0.9879639408647978)}\n",
        "    },\n",
        "    6: {\n",
        "        'clf': {'colsample_bytree': float(0.8447411578889518), 'gamma': float(0.6974693032602092), 'learning_rate': float(0.06842892970704363), 'max_depth': 9, 'n_estimators': 100, 'subsample': float(0.7529847965068651)},\n",
        "        'reg': {'alpha': float(0.05808361216819946), 'colsample_bytree': float(0.9464704583099741), 'lambda': float(0.6011150117432088), 'learning_rate': float(0.1516145155592091), 'max_depth': 8, 'n_estimators': 102, 'subsample': float(0.9879639408647978)}\n",
        "    },\n",
        "    12: {\n",
        "        'clf': {'colsample_bytree': float(0.8447411578889518), 'gamma': float(0.6974693032602092), 'learning_rate': float(0.06842892970704363), 'max_depth': 9, 'n_estimators': 100, 'subsample': float(0.7529847965068651)},\n",
        "        'reg': {'alpha': float(0.05808361216819946), 'colsample_bytree': float(0.9464704583099741), 'lambda': float(0.6011150117432088), 'learning_rate': float(0.1516145155592091), 'max_depth': 8, 'n_estimators': 102, 'subsample': float(0.9879639408647978)}\n",
        "    },\n",
        "    24: {\n",
        "        'clf': {'colsample_bytree': float(0.8447411578889518), 'gamma': float(0.6974693032602092), 'learning_rate': float(0.06842892970704363), 'max_depth': 9, 'n_estimators': 100, 'subsample': float(0.7529847965068651)},\n",
        "        'reg': {'alpha': float(0.230893825622149), 'colsample_bytree': float(0.6964101864104046), 'lambda': float(0.6832635188254582), 'learning_rate': float(0.13199933155652419), 'max_depth': 9, 'n_estimators': 84, 'subsample': float(0.9637281608315128)}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION FUNCTIONS (Identical to previous script)\n",
        "# ==========================================\n",
        "\n",
        "def load_file(full_path):\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Could not find file at: {full_path}\")\n",
        "    print(f\"Loading {full_path}...\")\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def prepare_base_features(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['datetime'] = pd.to_datetime(df_copy['datetime'], errors='coerce')\n",
        "    df_copy = df_copy.sort_values(['station_id', 'datetime'])\n",
        "\n",
        "    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "\n",
        "    for lag in [1, 3, 6]:\n",
        "        df_copy[f'temp_lag_{lag}'] = df_copy.groupby('station_id')['air_temp_c'].shift(lag)\n",
        "        df_copy[f'dew_lag_{lag}'] = df_copy.groupby('station_id')['dew_point_c'].shift(lag)\n",
        "\n",
        "    return df_copy.dropna()\n",
        "\n",
        "def generate_targets(df_base, h):\n",
        "    df = df_base.copy()\n",
        "    df = df.sort_values(['station_id', 'datetime'])\n",
        "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=h)\n",
        "\n",
        "    df['y_temp'] = df.groupby('station_id')['air_temp_c'].transform(\n",
        "        lambda x: x.rolling(window=indexer, min_periods=1).min()\n",
        "    )\n",
        "    df['y_event'] = (df['y_temp'] <= CONFIG['frost_threshold']).astype(int)\n",
        "    return df.dropna()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. VISUALIZATION FUNCTIONS (NEW)\n",
        "# ==========================================\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, h):\n",
        "    \"\"\"Plots the confusion matrix for the classification task.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Frost (0)', 'Frost (1)'],\n",
        "                yticklabels=['No Frost (0)', 'Frost (1)'])\n",
        "    plt.title(f'Confusion Matrix (h={h}h)')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show() # Display the plot\n",
        "\n",
        "\n",
        "def plot_roc_calibration(y_true, y_prob, h):\n",
        "    \"\"\"Plots the ROC Curve and Calibration Curve.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # --- ROC Curve ---\n",
        "    RocCurveDisplay.from_predictions(y_true, y_prob, name='XGBoost', ax=axes[0])\n",
        "    axes[0].plot([0, 1], [0, 1], linestyle='--', color='red', label='Chance')\n",
        "    axes[0].set_title(f'ROC Curve (h={h}h) - AUC: {roc_auc_score(y_true, y_prob):.4f}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # --- Calibration Curve (Reliability Diagram) ---\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_prob, n_bins=10)\n",
        "    axes[1].plot(mean_predicted_value, fraction_of_positives, marker='o', label='XGBoost')\n",
        "    axes[1].plot([0, 1], [0, 1], 'r--', label='Perfectly Calibrated')\n",
        "    axes[1].set_xlabel('Mean Predicted Probability')\n",
        "    axes[1].set_ylabel('Fraction of Positives (Actual Accuracy)')\n",
        "    axes[1].set_title(f'Calibration Curve (h={h}h) - Brier: {brier_score_loss(y_true, y_prob):.4f}')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display the plot\n",
        "\n",
        "\n",
        "def plot_regression_performance(y_true, y_pred, h):\n",
        "    \"\"\"Plots True vs. Predicted Temperature and Residuals.\"\"\"\n",
        "\n",
        "    # Calculate residuals\n",
        "    residuals = y_pred - y_true\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # --- True vs. Predicted ---\n",
        "    sns.scatterplot(x=y_true, y=y_pred, ax=axes[0], alpha=0.1, s=10)\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
        "    axes[0].set_xlabel('True Minimum Temperature (°C)')\n",
        "    axes[0].set_ylabel('Predicted Minimum Temperature (°C)')\n",
        "    axes[0].set_title(f'True vs. Predicted Temperature (h={h}h) - MAE: {mean_absolute_error(y_true, y_pred):.3f}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # --- Residual Distribution ---\n",
        "    sns.histplot(residuals, kde=True, ax=axes[1], bins=50)\n",
        "    axes[1].axvline(residuals.mean(), color='red', linestyle='--', label=f'Mean Bias: {residuals.mean():.3f}')\n",
        "    axes[1].set_xlabel('Prediction Residual (Predicted - True)')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].set_title(f'Residual Distribution (h={h}h)')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display the plot\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN PIPELINE (Full Dataset Training and Visualization)\n",
        "# ==========================================\n",
        "\n",
        "def run_pipeline_xgb_full_train_and_visualize():\n",
        "    \"\"\"Runs the full pipeline using the TUNED XGBoost models, trains on full data, and generates plots.\"\"\"\n",
        "\n",
        "    # A. Load base data\n",
        "    try:\n",
        "        train_base = load_file(CONFIG['train_file_path'])\n",
        "        test_base = load_file(CONFIG['test_file_path'])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # B. Prepare common features\n",
        "    print(\"Preparing base features (cyclical time, lags)...\")\n",
        "    train_processed_base = prepare_base_features(train_base)\n",
        "    test_processed_base = prepare_base_features(test_base)\n",
        "\n",
        "    all_summaries_xgb = []\n",
        "\n",
        "    # Loop through each horizon\n",
        "    for h_current in CONFIG['horizon']:\n",
        "        print(f\"\\nProcessing for horizon: {h_current} hours\")\n",
        "\n",
        "        # Get the TUNED parameters for the current horizon\n",
        "        params_h = TUNED_PARAMS.get(h_current)\n",
        "        if not params_h:\n",
        "            print(f\"ERROR: No tuned parameters found for horizon h={h_current}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Generate targets for the current horizon\n",
        "        print(f\"Generating targets for h={h_current}...\")\n",
        "        train_h_targets = generate_targets(train_processed_base, h_current)\n",
        "        test_h_targets = generate_targets(test_processed_base, h_current)\n",
        "\n",
        "        # Features\n",
        "        features = ['air_temp_c', 'rel_hum_percent', 'dew_point_c', 'wind_speed_m_s',\n",
        "                    'hour_sin', 'hour_cos', 'temp_lag_1', 'temp_lag_3', 'temp_lag_6']\n",
        "        features = [f for f in features if f in train_h_targets.columns]\n",
        "        print(f\"Training with features: {features}\")\n",
        "\n",
        "        # --- XGBoost Models (TUNED PARAMETERS, FULL TRAINING DATA) ---\n",
        "        print(\"\\n--- XGBoost Models (Full Training) ---\")\n",
        "\n",
        "        # 1. Frost Classifier\n",
        "        clf_params = params_h['clf'].copy()\n",
        "        clf_xgb = xgb.XGBClassifier(**clf_params, n_jobs=-1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "        print(\"Training Frost Classifier (XGBoost) on FULL data...\")\n",
        "        clf_xgb.fit(train_h_targets[features], train_h_targets['y_event'])\n",
        "\n",
        "        # 2. Temperature Regressor\n",
        "        reg_params = params_h['reg'].copy()\n",
        "        reg_xgb = xgb.XGBRegressor(**reg_params, n_jobs=-1, random_state=42)\n",
        "\n",
        "        print(\"Training Temperature Regressor (XGBoost) on FULL data...\")\n",
        "        reg_xgb.fit(train_h_targets[features], train_h_targets['y_temp'])\n",
        "\n",
        "        # 3. Generate predictions\n",
        "        print(\"Generating predictions (XGBoost)...\")\n",
        "        y_true_clf = test_h_targets['y_event']\n",
        "        y_prob_clf = clf_xgb.predict_proba(test_h_targets[features])[:, 1]\n",
        "        y_pred_clf = (y_prob_clf >= CONFIG['CONFUSION_THRESHOLD']).astype(int) # Apply threshold\n",
        "\n",
        "        y_true_reg = test_h_targets['y_temp']\n",
        "        y_pred_reg = reg_xgb.predict(test_h_targets[features])\n",
        "\n",
        "        # Store predictions for metrics\n",
        "        test_h_targets['p_event_xgb'] = y_prob_clf\n",
        "        test_h_targets['yhat_temp_xgb'] = y_pred_reg\n",
        "\n",
        "        # 4. Visualization\n",
        "        print(f\"\\n--- Generating Visualizations for h={h_current}h ---\")\n",
        "\n",
        "        print(\"Plotting Classification (Frost Event) Performance...\")\n",
        "        plot_confusion_matrix(y_true_clf, y_pred_clf, h_current)\n",
        "        plot_roc_calibration(y_true_clf, y_prob_clf, h_current)\n",
        "\n",
        "        print(\"Plotting Regression (Temperature) Performance...\")\n",
        "        plot_regression_performance(y_true_reg, y_pred_reg, h_current)\n",
        "\n",
        "        # 5. Calculate Metrics\n",
        "        results_xgb = test_h_targets.copy()\n",
        "        results_xgb['p_event'] = results_xgb['p_event_xgb']\n",
        "        results_xgb['yhat_temp'] = results_xgb['yhat_temp_xgb']\n",
        "        results_xgb['horizon_h'] = h_current\n",
        "        results_xgb = results_xgb.rename(columns={'datetime': 'timestamp'})\n",
        "\n",
        "        print(\"Calculating Final Metrics (XGBoost)...\")\n",
        "        summary_h_xgb = frost_metrics(results_xgb)\n",
        "        all_summaries_xgb.append(summary_h_xgb)\n",
        "\n",
        "    # Final Output\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"FINAL RESULTS FOR ALL HORIZONS (XGBoost - FULL TRAIN SET)\")\n",
        "    print(\"=\"*50)\n",
        "    final_summary_xgb = pd.concat(all_summaries_xgb).reset_index(drop=True)\n",
        "    print(final_summary_xgb.to_string(index=False))\n",
        "\n",
        "    return final_summary_xgb\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    final_summary_xgb = run_pipeline_xgb_full_train_and_visualize()"
      ]
    }
  ]
}