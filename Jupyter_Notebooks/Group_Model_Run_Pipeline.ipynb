{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpFWQV5ihUO7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import (\n",
        "    brier_score_loss, roc_auc_score, average_precision_score,\n",
        "    mean_absolute_error, mean_squared_error, make_scorer\n",
        ")\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# ==========================================\n",
        "# 1. METRICS & CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# --- Expected Calibration Error (ECE) ---\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
        "    \"\"\"\n",
        "    Helper for ECE calculation.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    binids = np.digitize(y_prob, bins) - 1\n",
        "\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        idx = binids == i\n",
        "        if np.any(idx):\n",
        "            acc_in_bin = y_true[idx].mean()\n",
        "            conf_in_bin = y_prob[idx].mean()\n",
        "            prop_in_bin = idx.mean()\n",
        "            ece += np.abs(acc_in_bin - conf_in_bin) * prop_in_bin\n",
        "    return ece\n",
        "\n",
        "def frost_metrics(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates the F3 Challenge Metric Table (Summary only for brevity).\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for h, g in df.groupby(\"horizon_h\", sort=True):\n",
        "        y, p = g[\"y_event\"].values, g[\"p_event\"].values\n",
        "\n",
        "        # Safety check for NaNs\n",
        "        mask = ~np.isnan(y) & ~np.isnan(p)\n",
        "        y, p = y[mask], p[mask]\n",
        "        g = g[mask]\n",
        "\n",
        "        if len(y) == 0: continue\n",
        "\n",
        "        # Classification Metrics\n",
        "        brier = brier_score_loss(y, np.clip(p, 0, 1))\n",
        "        ece   = expected_calibration_error(y, p, n_bins=10)\n",
        "\n",
        "        if len(np.unique(y)) < 2:\n",
        "            roc = np.nan\n",
        "            pr   = np.nan\n",
        "        else:\n",
        "            try: roc = roc_auc_score(y, p)\n",
        "            except: roc = np.nan\n",
        "            try: pr  = average_precision_score(y, p)\n",
        "            except: pr  = np.nan\n",
        "\n",
        "        # Regression Metrics\n",
        "        mae  = float(mean_absolute_error(g[\"y_temp\"], g[\"yhat_temp\"]))\n",
        "        rmse = float(np.sqrt(mean_squared_error(g[\"y_temp\"], g[\"yhat_temp\"])))\n",
        "        bias = float((g[\"yhat_temp\"] - g[\"y_temp\"]).mean())\n",
        "\n",
        "        rows.append({\n",
        "            \"horizon_h\": int(h),\n",
        "            \"brier\": brier, \"ece\": ece, \"roc_auc\": roc, \"pr_auc\": pr,\n",
        "            \"mae\": mae, \"rmse\": rmse, \"bias\": bias,\n",
        "            \"n_samples\": int(len(g))\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values(\"horizon_h\") # Returns only the summary\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG = {\n",
        "    'horizon': [3, 6, 12, 24],\n",
        "    'frost_threshold': 0.0,\n",
        "    # NOTE: Update these paths if your files are located elsewhere\n",
        "    'train_file_path': '/content/train_set_filled_w_Mean_cleaned.csv',\n",
        "    'test_file_path': '/content/test_set_filled_w_Mean_cleaned.csv',\n",
        "    'TUNING_SAMPLE_FRACTION': 0.1, # 10% sample for tuning\n",
        "    'N_TUNING_ITERATIONS': 10,     # Number of parameter settings to sample\n",
        "    'N_CV_FOLDS': 3                # Number of cross-validation folds\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def load_file(full_path):\n",
        "    \"\"\"Loads a file given its full path.\"\"\"\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Could not find file at: {full_path}\")\n",
        "    print(f\"Loading {full_path}...\")\n",
        "    # Infer file type (simplified from original)\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def prepare_base_features(df):\n",
        "    \"\"\"Prepares horizon-independent features (cyclical time, lags).\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['datetime'] = pd.to_datetime(df_copy['datetime'], errors='coerce')\n",
        "    df_copy = df_copy.sort_values(['station_id', 'datetime'])\n",
        "\n",
        "    # Cyclical Time\n",
        "    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "\n",
        "    # Lag Features\n",
        "    for lag in [1, 3, 6]:\n",
        "        df_copy[f'temp_lag_{lag}'] = df_copy.groupby('station_id')['air_temp_c'].shift(lag)\n",
        "        df_copy[f'dew_lag_{lag}'] = df_copy.groupby('station_id')['dew_point_c'].shift(lag)\n",
        "\n",
        "    return df_copy.dropna()\n",
        "\n",
        "def generate_targets(df_base, h):\n",
        "    \"\"\"Generates horizon-dependent targets (y_temp, y_event) for horizon h.\"\"\"\n",
        "    df = df_base.copy()\n",
        "    df = df.sort_values(['station_id', 'datetime'])\n",
        "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=h)\n",
        "\n",
        "    # Look H hours ahead for the minimum temp\n",
        "    df['y_temp'] = df.groupby('station_id')['air_temp_c'].transform(\n",
        "        lambda x: x.rolling(window=indexer, min_periods=1).min()\n",
        "    )\n",
        "    df['y_event'] = (df['y_temp'] <= CONFIG['frost_threshold']).astype(int)\n",
        "    return df.dropna()\n",
        "\n",
        "# ==========================================\n",
        "# 3. XGBOOST TUNING FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def tune_xgboost(X_train, y_clf, y_reg, features):\n",
        "    \"\"\"\n",
        "    Performs RandomizedSearchCV for both XGBoost Classifier and Regressor.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting XGBoost Hyperparameter Tuning (Randomized Search) ---\")\n",
        "\n",
        "    # 1. Classifier (Frost Event) Tuning\n",
        "    print(\"Tuning Frost Classifier...\")\n",
        "    clf_xgb = xgb.XGBClassifier(n_jobs=-1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "    clf_param_dist = {\n",
        "        'n_estimators': randint(50, 200),\n",
        "        'max_depth': randint(3, 10),\n",
        "        'learning_rate': uniform(0.01, 0.2),\n",
        "        'subsample': uniform(0.6, 0.4),\n",
        "        'colsample_bytree': uniform(0.6, 0.4),\n",
        "        'gamma': uniform(0, 5)\n",
        "    }\n",
        "\n",
        "    clf_search = RandomizedSearchCV(\n",
        "        estimator=clf_xgb,\n",
        "        param_distributions=clf_param_dist,\n",
        "        n_iter=CONFIG['N_TUNING_ITERATIONS'],\n",
        "        scoring='roc_auc',\n",
        "        cv=CONFIG['N_CV_FOLDS'],\n",
        "        verbose=0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    clf_search.fit(X_train[features], y_clf)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"Classifier Tuning finished in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best Classifier Params: {clf_search.best_params_}\")\n",
        "    best_clf = clf_search.best_estimator_\n",
        "\n",
        "    # 2. Regressor (Temperature) Tuning\n",
        "    print(\"\\nTuning Temperature Regressor...\")\n",
        "    reg_xgb = xgb.XGBRegressor(n_jobs=-1, random_state=42)\n",
        "\n",
        "    reg_param_dist = {\n",
        "        'n_estimators': randint(50, 200),\n",
        "        'max_depth': randint(3, 10),\n",
        "        'learning_rate': uniform(0.01, 0.2),\n",
        "        'subsample': uniform(0.6, 0.4),\n",
        "        'colsample_bytree': uniform(0.6, 0.4),\n",
        "        'lambda': uniform(0, 1),\n",
        "        'alpha': uniform(0, 1)\n",
        "    }\n",
        "\n",
        "    # Use Negative MAE as the scoring metric for regression\n",
        "    neg_mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "    reg_search = RandomizedSearchCV(\n",
        "        estimator=reg_xgb,\n",
        "        param_distributions=reg_param_dist,\n",
        "        n_iter=CONFIG['N_TUNING_ITERATIONS'],\n",
        "        scoring=neg_mae_scorer,\n",
        "        cv=CONFIG['N_CV_FOLDS'],\n",
        "        verbose=0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    reg_search.fit(X_train[features], y_reg)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"Regressor Tuning finished in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best Regressor Params: {reg_search.best_params_}\")\n",
        "    best_reg = reg_search.best_estimator_\n",
        "\n",
        "    return best_clf, best_reg\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN PIPELINE (XGBoost Only)\n",
        "# ==========================================\n",
        "\n",
        "def run_pipeline_xgb_only():\n",
        "    \"\"\"Runs the full pipeline using only XGBoost models with tuning.\"\"\"\n",
        "    # A. Load base data\n",
        "    try:\n",
        "        train_base = load_file(CONFIG['train_file_path'])\n",
        "        test_base = load_file(CONFIG['test_file_path'])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # B. Prepare common features\n",
        "    print(\"Preparing base features (cyclical time, lags)...\")\n",
        "    train_processed_base = prepare_base_features(train_base)\n",
        "    test_processed_base = prepare_base_features(test_base)\n",
        "\n",
        "    all_summaries_xgb = []\n",
        "\n",
        "    # Loop through each horizon\n",
        "    for h_current in CONFIG['horizon']:\n",
        "        print(f\"\\nProcessing for horizon: {h_current} hours\")\n",
        "\n",
        "        # Generate targets for the current horizon\n",
        "        print(f\"Generating targets for h={h_current}...\")\n",
        "        train_h_targets = generate_targets(train_processed_base, h_current)\n",
        "        test_h_targets = generate_targets(test_processed_base, h_current)\n",
        "\n",
        "        # Features\n",
        "        features = ['air_temp_c', 'rel_hum_percent', 'dew_point_c', 'wind_speed_m_s',\n",
        "                    'hour_sin', 'hour_cos', 'temp_lag_1', 'temp_lag_3', 'temp_lag_6']\n",
        "        features = [f for f in features if f in train_h_targets.columns]\n",
        "        print(f\"Training with features: {features}\")\n",
        "\n",
        "        # --- Subsample the training data for faster tuning ---\n",
        "        if CONFIG['TUNING_SAMPLE_FRACTION'] < 1.0:\n",
        "            print(f\"Sampling {CONFIG['TUNING_SAMPLE_FRACTION'] * 100:.0f}% of training data for XGBoost tuning.\")\n",
        "            train_subsample = train_h_targets.sample(frac=CONFIG['TUNING_SAMPLE_FRACTION'], random_state=42)\n",
        "        else:\n",
        "            train_subsample = train_h_targets.copy()\n",
        "\n",
        "        # --- XGBoost Models (TUNED) ---\n",
        "        best_clf_xgb, best_reg_xgb = tune_xgboost(\n",
        "            train_subsample,\n",
        "            train_subsample['y_event'],\n",
        "            train_subsample['y_temp'],\n",
        "            features\n",
        "        )\n",
        "\n",
        "        # 1. Train the final models on the FULL training set\n",
        "        # Although tuning was done on a subsample, we should re-train the final chosen parameters\n",
        "        # on the full dataset before predicting on the test set for best performance.\n",
        "        # However, for maximum speed, we will stick to the tuned model trained on the subsample.\n",
        "        # If performance is key, uncomment the lines below and use full train_h_targets.\n",
        "        # print(\"\\nRe-training final models on FULL training set...\")\n",
        "        # best_clf_xgb.fit(train_h_targets[features], train_h_targets['y_event'])\n",
        "        # best_reg_xgb.fit(train_h_targets[features], train_h_targets['y_temp'])\n",
        "\n",
        "        # 2. Generate predictions\n",
        "        print(\"Generating predictions (XGBoost) using best models...\")\n",
        "        test_h_targets['p_event_xgb'] = best_clf_xgb.predict_proba(test_h_targets[features])[:, 1]\n",
        "        test_h_targets['yhat_temp_xgb'] = best_reg_xgb.predict(test_h_targets[features])\n",
        "\n",
        "        # 3. Formatting and Metrics\n",
        "        results_xgb = test_h_targets.copy()\n",
        "        results_xgb['p_event'] = results_xgb['p_event_xgb']\n",
        "        results_xgb['yhat_temp'] = results_xgb['yhat_temp_xgb']\n",
        "        results_xgb['horizon_h'] = h_current\n",
        "        results_xgb = results_xgb.rename(columns={'datetime': 'timestamp'})\n",
        "\n",
        "        print(\"Calculating Final Metrics (XGBoost)...\")\n",
        "        summary_h_xgb = frost_metrics(results_xgb)\n",
        "        all_summaries_xgb.append(summary_h_xgb)\n",
        "\n",
        "    # Final Output\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"FINAL RESULTS FOR ALL HORIZONS (XGBoost - TUNED)\")\n",
        "    print(\"=\"*50)\n",
        "    final_summary_xgb = pd.concat(all_summaries_xgb).reset_index(drop=True)\n",
        "    print(final_summary_xgb.to_string(index=False))\n",
        "\n",
        "    return final_summary_xgb\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   final_summary_xgb = run_pipeline_xgb_only()\n"
      ]
    }
  ]
}