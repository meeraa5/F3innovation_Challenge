{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpFWQV5ihUO7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ==========================================\n",
        "# 1. METRICS & CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "def frost_metrics_regression_only(df: pd.DataFrame):\n",
        "    \"\"\"Calculates MAE, RMSE, and Bias for the combined predictions.\"\"\"\n",
        "    y_true = df[\"y_temp\"].values\n",
        "    y_pred = df[\"yhat_temp\"].values\n",
        "\n",
        "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
        "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
        "\n",
        "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    bias = float((y_pred - y_true).mean())\n",
        "\n",
        "    return {\"mae\": mae, \"rmse\": rmse, \"bias\": bias, \"n_samples\": len(y_true)}\n",
        "\n",
        "CONFIG = {\n",
        "    'horizon': [3, 6, 12, 24],\n",
        "    'frost_threshold': 0.0,\n",
        "    # NOTE: Assuming you have access to both original files\n",
        "    'train_file_path': '/content/train_set_filled_w_Mean_cleaned.csv',\n",
        "    'test_file_path': '/content/test_set_filled_w_Mean_cleaned.csv',\n",
        "}\n",
        "\n",
        "# --- Tuned Hyperparameters (Focusing on Regression) ---\n",
        "# We will use the best regressor params found previously for simplicity.\n",
        "TUNED_REG_PARAMS = {\n",
        "    3: {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796},\n",
        "    6: {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796},\n",
        "    12: {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796},\n",
        "    24: {'alpha': 0.23089, 'colsample_bytree': 0.69641, 'lambda': 0.68326, 'learning_rate': 0.13199, 'max_depth': 9, 'n_estimators': 84, 'subsample': 0.96372}\n",
        "}\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def load_file(full_path):\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Could not find file at: {full_path}\")\n",
        "    return pd.read_csv(full_path)\n",
        "\n",
        "def prepare_base_features(df):\n",
        "    \"\"\"Prepares horizon-independent features (cyclical time, lags).\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['datetime'] = pd.to_datetime(df_copy['datetime'], errors='coerce')\n",
        "    df_copy = df_copy.sort_values(['station_id', 'datetime'])\n",
        "\n",
        "    # Cyclical Time\n",
        "    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['datetime'].dt.hour / 24)\n",
        "\n",
        "    # Lag Features\n",
        "    for lag in [1, 3, 6]:\n",
        "        df_copy[f'temp_lag_{lag}'] = df_copy.groupby('station_id')['air_temp_c'].shift(lag)\n",
        "        df_copy[f'dew_lag_{lag}'] = df_copy.groupby('station_id')['dew_point_c'].shift(lag)\n",
        "\n",
        "    return df_copy.dropna()\n",
        "\n",
        "def generate_targets(df_base, h):\n",
        "    \"\"\"Generates horizon-dependent targets (y_temp) for horizon h.\"\"\"\n",
        "    df = df_base.copy()\n",
        "    df = df.sort_values(['station_id', 'datetime'])\n",
        "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=h)\n",
        "\n",
        "    # Look H hours ahead for the minimum temp\n",
        "    df['y_temp'] = df.groupby('station_id')['air_temp_c'].transform(\n",
        "        lambda x: x.rolling(window=indexer, min_periods=1).min()\n",
        "    )\n",
        "    # We only need y_temp for the regression generalization test\n",
        "    return df.dropna().reset_index(drop=True)\n",
        "\n",
        "# ==========================================\n",
        "# 3. LOOCV PIPELINE\n",
        "# ==========================================\n",
        "\n",
        "def run_pipeline_leave_one_station_out():\n",
        "    \"\"\"Runs the full pipeline using the tuned XGBoost Regressor in a LOOCV fashion.\"\"\"\n",
        "\n",
        "    # A. Load and Combine Data\n",
        "    try:\n",
        "        train_base = load_file(CONFIG['train_file_path'])\n",
        "        test_base = load_file(CONFIG['test_file_path'])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Combine data and prepare features across the entire set\n",
        "    print(\"Combining train and test data...\")\n",
        "    full_data = pd.concat([train_base, test_base], ignore_index=True)\n",
        "    full_processed_base = prepare_base_features(full_data)\n",
        "\n",
        "    all_stations = full_processed_base['station_id'].unique()\n",
        "    num_stations = len(all_stations)\n",
        "    print(f\"Found {num_stations} unique stations: {all_stations}\")\n",
        "\n",
        "    # Features list\n",
        "    features = ['air_temp_c', 'rel_hum_percent', 'dew_point_c', 'wind_speed_m_s',\n",
        "                'hour_sin', 'hour_cos', 'temp_lag_1', 'temp_lag_3', 'temp_lag_6']\n",
        "\n",
        "    all_generalization_results = []\n",
        "\n",
        "    # Loop through each horizon\n",
        "    for h_current in CONFIG['horizon']:\n",
        "        print(f\"\\n{'='*60}\\nProcessing for horizon: {h_current} hours\\n{'='*60}\")\n",
        "\n",
        "        # Get Regressor Params\n",
        "        reg_params = TUNED_REG_PARAMS.get(h_current)\n",
        "        if not reg_params: continue\n",
        "\n",
        "        # Generate targets for the current horizon on the full data\n",
        "        data_h_targets = generate_targets(full_processed_base, h_current)\n",
        "\n",
        "        station_predictions = []\n",
        "\n",
        "        # --- Leave-One-Station-Out Loop ---\n",
        "        for i, station_to_test in enumerate(all_stations):\n",
        "            print(f\"|--- Iteration {i+1}/{num_stations}: Leaving out Station {station_to_test} for testing.\")\n",
        "\n",
        "            # Split data\n",
        "            train_set = data_h_targets[data_h_targets['station_id'] != station_to_test]\n",
        "            test_set = data_h_targets[data_h_targets['station_id'] == station_to_test]\n",
        "\n",
        "            X_train, y_train = train_set[features], train_set['y_temp']\n",
        "            X_test, y_test = test_set[features], test_set['y_temp']\n",
        "\n",
        "            # 1. Initialize and Train Regressor\n",
        "            reg_xgb = xgb.XGBRegressor(**reg_params, n_jobs=-1, random_state=42)\n",
        "\n",
        "            # Train on 16 stations\n",
        "            reg_xgb.fit(X_train, y_train)\n",
        "\n",
        "            # 2. Predict on the unseen station (the 17th station)\n",
        "            y_pred = reg_xgb.predict(X_test)\n",
        "\n",
        "            # 3. Store results for the unseen station\n",
        "            result_df = pd.DataFrame({\n",
        "                'station_id': station_to_test,\n",
        "                'horizon_h': h_current,\n",
        "                'y_temp': y_test.values,\n",
        "                'yhat_temp': y_pred\n",
        "            })\n",
        "            station_predictions.append(result_df)\n",
        "\n",
        "            # Calculate metrics for the single unseen station\n",
        "            metrics_single = frost_metrics_regression_only(result_df)\n",
        "            print(f\"|--- Station {station_to_test} Generalization MAE: {metrics_single['mae']:.4f}\")\n",
        "\n",
        "        # --- Aggregate and Final Metrics for the Horizon ---\n",
        "        combined_results = pd.concat(station_predictions, ignore_index=True)\n",
        "        final_metrics = frost_metrics_regression_only(combined_results)\n",
        "        final_metrics['horizon_h'] = h_current\n",
        "        final_metrics['test_type'] = 'Generalization (LOOCV)'\n",
        "        all_generalization_results.append(final_metrics)\n",
        "\n",
        "    # Final Output\n",
        "    final_df = pd.DataFrame(all_generalization_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL RESULTS: XGBOOST REGRESSION GENERALIZATION (LEAVE-ONE-STATION-OUT)\")\n",
        "    print(f\"Test Type: Predicting on a fully UNSEEN Station for each iteration.\")\n",
        "    print(\"=\"*70)\n",
        "    print(final_df[['horizon_h', 'mae', 'rmse', 'bias', 'n_samples']].to_string(index=False))\n",
        "\n",
        "    return final_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    final_summary_xgb_loocv = run_pipeline_leave_one_station_out()"
      ]
    }
  ]
}