{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4f6ccd-23b0-40f2-a550-2d1bede83fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.12/site-packages (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.12/site-packages (from xgboost) (2.28.9)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from xgboost) (1.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "############################################################\n",
      "PHASE 1: DATA LOADING AND PREPROCESSING\n",
      "############################################################\n",
      "--- 1. Loading and initial cleanup ---\n",
      "  - üîÑ Decompressing cimis_all_stations.csv.gz to cimis_all_stations_unzipped.csv...\n",
      "  - ‚úÖ Decompression complete.\n",
      "  - Initial DataFrame size: 2367360 rows\n",
      "--- 2. Reindexing and filling hourly gaps ---\n",
      "  - DataFrame size after reindexing: 2367360 rows\n",
      "--- 3. Final cleaning and feature engineering ---\n",
      "--- 4. Generating lag features ---\n",
      "  - DataFrame size after lag generation and dropping initial NaNs: 2367252 rows\n",
      "--- 5. Generating targets for model training ---\n",
      "  - Processed DataFrame size for horizon 3h: 2367252 rows\n",
      "  - Processed DataFrame size for horizon 6h: 2367252 rows\n",
      "  - Processed DataFrame size for horizon 12h: 2367252 rows\n",
      "  - Processed DataFrame size for horizon 24h: 2367252 rows\n",
      "\n",
      "############################################################\n",
      "PHASE 2: MODEL TRAINING\n",
      "############################################################\n",
      "\n",
      "==================================================\n",
      "INITIATING FINAL XGBOOST MODEL TRAINING\n",
      "==================================================\n",
      "\n",
      "Training for horizon: 3 hours...\n",
      "  - Training Classifier...\n",
      "  - Training Regressor...\n",
      "\n",
      "Training for horizon: 6 hours...\n",
      "  - Training Classifier...\n",
      "  - Training Regressor...\n",
      "\n",
      "Training for horizon: 12 hours...\n",
      "  - Training Classifier...\n",
      "  - Training Regressor...\n",
      "\n",
      "Training for horizon: 24 hours...\n",
      "  - Training Classifier...\n",
      "  - Training Regressor...\n",
      "\n",
      "‚úÖ All final models trained successfully.\n",
      "\n",
      "############################################################\n",
      "PHASE 3: STARTING INTERACTIVE PREDICTION\n",
      "############################################################\n",
      "\n",
      "==================================================\n",
      "INTERACTIVE WEATHER PREDICTOR\n",
      "==================================================\n",
      "Available Stations: [2, 7, 15, 39, 47, 70, 71, 80, 105, 124, 125, 131, 146, 182, 194, 195, 205, 206]\n",
      "Data range: 2010-09-28 07:00 to 2025-09-29 00:00\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Station ID (2-206) or -1 to exit:  80\n",
      "Enter Date and Hour (YYYY-MM-DD HH):  2013-02-01 03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CURRENT CONDITIONS FOR STATION 80 AT 2013-02-01 03:00:00+00:00:\n",
      "  Air Temp:       3.60 ¬∞C\n",
      "  Relative Hum:   94.0 %\n",
      "  Dew Point:      2.80 ¬∞C\n",
      "  Wind Speed:     0.50 m/s\n",
      "  --- Lagged Temp: 8.70 ¬∞C (6h ago)\n",
      "==================================================\n",
      "\n",
      "FORECASTED MINIMUM TEMPERATURE & FROST RISK:\n",
      "| H=3h Forecast:\n",
      "|   Min Temp: 2.99 ¬∞C\n",
      "|   Frost Prob: 0.00 -> LOW RISK (No Frost)\n",
      "| H=6h Forecast:\n",
      "|   Min Temp: 2.50 ¬∞C\n",
      "|   Frost Prob: 0.00 -> LOW RISK (No Frost)\n",
      "| H=12h Forecast:\n",
      "|   Min Temp: 2.53 ¬∞C\n",
      "|   Frost Prob: 0.01 -> LOW RISK (No Frost)\n",
      "| H=24h Forecast:\n",
      "|   Min Temp: 2.49 ¬∞C\n",
      "|   Frost Prob: 0.01 -> LOW RISK (No Frost)\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Station ID (2-206) or -1 to exit:  -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting interactive prediction.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import datetime\n",
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION AND HYPERPARAMETERS\n",
    "# ==========================================\n",
    "\n",
    "# --- File Paths (Updated to reference the .gz file) ---\n",
    "CIMIS_GZ_FILE_NAME = 'cimis_all_stations.csv.gz' \n",
    "CIMIS_CSV_FILE_NAME = 'cimis_all_stations_unzipped.csv' # Temp file for unzipped data\n",
    "\n",
    "# --- Model Configuration ---\n",
    "CONFIG = {\n",
    "    'horizon': [3, 6, 12, 24],\n",
    "    'frost_threshold': 0.0,\n",
    "    'FEATURES': [\n",
    "        'air_temp_c', 'rel_hum_percent', 'dew_point_c', 'wind_speed_m_s',\n",
    "        'hour_sin', 'hour_cos', 'temp_lag_1', 'temp_lag_3', 'temp_lag_6'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Tuned Hyperparameters ---\n",
    "# (TUNED_PARAMS remains the same)\n",
    "TUNED_PARAMS = {\n",
    "    3: {\n",
    "        'clf': {'colsample_bytree': 0.99329, 'gamma': 2.33381, 'learning_rate': 0.18198, 'max_depth': 9, 'n_estimators': 70, 'subsample': 0.78019},\n",
    "        'reg': {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796}\n",
    "    },\n",
    "    6: {\n",
    "        'clf': {'colsample_bytree': 0.84474, 'gamma': 0.69746, 'learning_rate': 0.06842, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.75298},\n",
    "        'reg': {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796}\n",
    "    },\n",
    "    12: {\n",
    "        'clf': {'colsample_bytree': 0.84474, 'gamma': 0.69746, 'learning_rate': 0.06842, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.75298},\n",
    "        'reg': {'alpha': 0.05808, 'colsample_bytree': 0.94647, 'lambda': 0.60111, 'learning_rate': 0.15161, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.98796}\n",
    "    },\n",
    "    24: {\n",
    "        'clf': {'colsample_bytree': 0.84474, 'gamma': 0.69746, 'learning_rate': 0.06842, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.75298},\n",
    "        'reg': {'alpha': 0.23089, 'colsample_bytree': 0.69641, 'lambda': 0.68326, 'learning_rate': 0.13199, 'max_depth': 9, 'n_estimators': 84, 'subsample': 0.96372}\n",
    "    }\n",
    "}\n",
    "\n",
    "def sanitize_global_params(params):\n",
    "    \"\"\"Recursively checks all float values in the nested dicts and removes any NaNs.\"\"\"\n",
    "    cleaned_params = {}\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, dict):\n",
    "            cleaned_params[k] = sanitize_global_params(v)\n",
    "        elif isinstance(v, (int, float)):\n",
    "            v_float = float(v)\n",
    "            if np.isnan(v_float):\n",
    "                print(f\"  - ‚ö†Ô∏è Global Sanitizer: Dropping NaN parameter: {k}\")\n",
    "                continue\n",
    "            cleaned_params[k] = v_float\n",
    "        else:\n",
    "            cleaned_params[k] = v\n",
    "    return cleaned_params\n",
    "\n",
    "TUNED_PARAMS = sanitize_global_params(TUNED_PARAMS)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA PREPARATION FUNCTIONS (Updated Loading)\n",
    "# ==========================================\n",
    "\n",
    "def load_and_preprocess_data(gz_file_name, csv_file_name):\n",
    "    \"\"\"\n",
    "    Loads, fills gaps, cleans, and adds necessary features to the raw CIMIS data.\n",
    "    This version explicitly unzips the .gz file to a temporary .csv file first.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--- 1. Loading and initial cleanup ---\")\n",
    "    \n",
    "    # --- Explicit Gzip Decompression ---\n",
    "    print(f\"  - üîÑ Decompressing {gz_file_name} to {csv_file_name}...\")\n",
    "    try:\n",
    "        # 'rt' opens in read mode for text\n",
    "        with gzip.open(gz_file_name, 'rt', encoding='utf-8') as f_in:\n",
    "            with open(csv_file_name, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        print(\"  - ‚úÖ Decompression complete.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - üî¥ Error: Compressed file not found at: {os.path.abspath(gz_file_name)}\")\n",
    "        return pd.DataFrame(), {}\n",
    "    except Exception as e:\n",
    "        print(f\"  - üî¥ Error during decompression: {e}\")\n",
    "        return pd.DataFrame(), {}\n",
    "        \n",
    "    # --- Read the unzipped CSV file ---\n",
    "    df = pd.read_csv(csv_file_name) \n",
    "    # Clean up the unzipped file immediately\n",
    "    os.remove(csv_file_name)\n",
    "    print(f\"  - Initial DataFrame size: {len(df)} rows\")\n",
    "\n",
    "    # Define a comprehensive rename map for all relevant columns at once\n",
    "    rename_map = {\n",
    "        \"Stn Id\": \"station_id\",\n",
    "        \"Rel Hum (%)\": \"rel_hum_percent\",\n",
    "        \"Dew Point (C)\": \"dew_point_c\",\n",
    "        \"Wind Speed (m/s)\": \"wind_speed_m_s\",\n",
    "        \"Air Temp (C)\": \"air_temp_c\",\n",
    "        \"Stn Name\": \"station_name\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map, errors='ignore')\n",
    "\n",
    "    # Initial date parsing and cleanup\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True)\n",
    "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # Create timestamp column\n",
    "    hour_int = df[\"Hour (PST)\"].astype(int)\n",
    "    hour = hour_int // 100\n",
    "    df[\"datetime\"] = df[\"Date\"].dt.floor(\"D\") + pd.to_timedelta(hour, unit=\"h\")\n",
    "\n",
    "    # --- 2. Reindex to continuous hourly timestamps and fill gaps ---\n",
    "    print(\"--- 2. Reindexing and filling hourly gaps ---\")\n",
    "\n",
    "    def reindex_station_to_hourly(group):\n",
    "        current_station_id = group.name\n",
    "        group = group.sort_values(\"datetime\")\n",
    "        full_idx = pd.date_range(start=group[\"datetime\"].min(), end=group[\"datetime\"].max(), freq=\"h\")\n",
    "        g = group.set_index(\"datetime\").reindex(full_idx)\n",
    "        g.index.name = \"datetime\"\n",
    "        for col in [\"station_name\", \"CIMIS Region\"]:\n",
    "            if col in group.columns and not group[col].isnull().all():\n",
    "                g[col] = g[col].fillna(group[col].iloc[0])\n",
    "        g['station_id'] = current_station_id\n",
    "        return g\n",
    "\n",
    "    df_full = (\n",
    "        df.groupby(\"station_id\", group_keys=False) \n",
    "        .apply(reindex_station_to_hourly, include_groups=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(f\"  - DataFrame size after reindexing: {len(df_full)} rows\")\n",
    "\n",
    "    # --- 3. Final Cleaning and Feature Engineering ---\n",
    "    print(\"--- 3. Final cleaning and feature engineering ---\")\n",
    "\n",
    "    df_clean = df_full.copy()\n",
    "    qc_cols = [c for c in df_clean.columns if \"qc\" in c.lower()]\n",
    "    df_clean = df_clean.drop(columns=qc_cols, errors='ignore')\n",
    "\n",
    "    for c in df_clean.columns:\n",
    "        if c not in [\"station_name\", \"CIMIS Region\", \"datetime\"]:\n",
    "            df_clean[c] = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
    "\n",
    "    numeric_cols = df_clean.select_dtypes(include=\"number\").columns\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n",
    "\n",
    "    df_clean[\"month\"] = df_clean[\"datetime\"].dt.month\n",
    "    df_clean[\"hour\"] = df_clean[\"datetime\"].dt.hour\n",
    "    df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['datetime'].dt.hour / 24)\n",
    "    df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['datetime'].dt.hour / 24)\n",
    "\n",
    "    # --- 4. Generate Lag Features on Full Data ---\n",
    "    print(\"--- 4. Generating lag features ---\")\n",
    "    df_clean = df_clean.sort_values(['station_id', 'datetime'])\n",
    "    for lag in [1, 3, 6]:\n",
    "        df_clean[f'temp_lag_{lag}'] = df_clean.groupby('station_id')['air_temp_c'].shift(lag)\n",
    "        df_clean[f'dew_lag_{lag}'] = df_clean.groupby('station_id')['dew_point_c'].shift(lag)\n",
    "\n",
    "    df_clean = df_clean.dropna(subset=['temp_lag_6', 'dew_lag_6']).reset_index(drop=True)\n",
    "    print(f\"  - DataFrame size after lag generation and dropping initial NaNs: {len(df_clean)} rows\")\n",
    "\n",
    "    # --- 5. Generate Targets (for model training) ---\n",
    "    print(\"--- 5. Generating targets for model training ---\")\n",
    "\n",
    "    train_data_by_horizon = {}\n",
    "    for h in CONFIG['horizon']:\n",
    "        df_h = df_clean.copy()\n",
    "        df_h = df_h.sort_values(['station_id', 'datetime'])\n",
    "        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=h) \n",
    "\n",
    "        df_h['y_temp'] = df_h.groupby('station_id')['air_temp_c'].transform(\n",
    "             lambda x: x.rolling(window=indexer, min_periods=1).min()\n",
    "        )\n",
    "        df_h['y_event'] = (df_h['y_temp'] <= CONFIG['frost_threshold']).astype(int)\n",
    "\n",
    "        df_h_processed = df_h.dropna(subset=['y_temp']).reset_index(drop=True)\n",
    "\n",
    "        if df_h_processed.empty:\n",
    "            print(f\"  - ‚ö†Ô∏è Warning: No training data available for horizon {h} after dropping NaNs. Skipping this horizon.\")\n",
    "        else:\n",
    "            train_data_by_horizon[h] = df_h_processed\n",
    "            print(f\"  - Processed DataFrame size for horizon {h}h: {len(df_h_processed)} rows\")\n",
    "\n",
    "    return df_clean, train_data_by_horizon\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL TRAINING AND STORAGE\n",
    "# ==========================================\n",
    "\n",
    "def sanitize_params(params):\n",
    "    \"\"\"LOCAL CLEANUP FUNCTION: Ensures parameters are correctly formatted for XGBoost.\"\"\"\n",
    "    cleaned_params = {}\n",
    "    INT_PARAMS = ['n_estimators', 'max_depth']\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            if np.isnan(float(v)):\n",
    "                continue\n",
    "            if k in INT_PARAMS:\n",
    "                try:\n",
    "                    cleaned_params[k] = int(v)\n",
    "                except ValueError:\n",
    "                    print(f\"  - üî¥ Error: Could not convert '{k}' value '{v}' to integer. Dropping parameter.\")\n",
    "                    continue\n",
    "            else:\n",
    "                cleaned_params[k] = v\n",
    "        else:\n",
    "            cleaned_params[k] = v\n",
    "    return cleaned_params\n",
    "\n",
    "\n",
    "def train_final_models(train_data_by_horizon):\n",
    "    \"\"\"Trains and returns a dictionary of tuned XGBoost models for each horizon.\"\"\"\n",
    "\n",
    "    trained_models = {h: {'clf': None, 'reg': None} for h in CONFIG['horizon']}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INITIATING FINAL XGBOOST MODEL TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for h_current in CONFIG['horizon']:\n",
    "        if h_current not in train_data_by_horizon or train_data_by_horizon[h_current].empty:\n",
    "            print(f\"Skipping training for horizon: {h_current} hours, no data available.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTraining for horizon: {h_current} hours...\")\n",
    "\n",
    "        train_h_targets = train_data_by_horizon[h_current]\n",
    "        params_h = TUNED_PARAMS[h_current]\n",
    "        features = CONFIG['FEATURES']\n",
    "\n",
    "        base_score_clf = train_h_targets['y_event'].mean()\n",
    "\n",
    "        # 1. Frost Classifier\n",
    "        clf_params = sanitize_params(params_h['clf'].copy())\n",
    "        clf_xgb = xgb.XGBClassifier(\n",
    "            **clf_params,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            # Note: use_label_encoder=False is standard practice in recent XGBoost versions\n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss',\n",
    "            base_score=base_score_clf,\n",
    "            verbosity=0\n",
    "        )\n",
    "        print(\"  - Training Classifier...\")\n",
    "        clf_xgb.fit(train_h_targets[features], train_h_targets['y_event'])\n",
    "        trained_models[h_current]['clf'] = clf_xgb\n",
    "\n",
    "        # 2. Temperature Regressor\n",
    "        reg_params = sanitize_params(params_h['reg'].copy())\n",
    "        reg_xgb = xgb.XGBRegressor(\n",
    "            **reg_params, \n",
    "            n_jobs=-1, \n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "        print(\"  - Training Regressor...\")\n",
    "        reg_xgb.fit(train_h_targets[features], train_h_targets['y_temp'])\n",
    "        trained_models[h_current]['reg'] = reg_xgb\n",
    "\n",
    "    print(\"\\n‚úÖ All final models trained successfully.\")\n",
    "    return trained_models\n",
    "\n",
    "# ==========================================\n",
    "# 3. INTERACTIVE PREDICTION\n",
    "# ==========================================\n",
    "\n",
    "def interactive_prediction(df_full, trained_models):\n",
    "    \"\"\"Prompts user for input and predicts frost/temp for all horizons.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE WEATHER PREDICTOR\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Get available stations and date range\n",
    "    stations = sorted(df_full['station_id'].unique().tolist())\n",
    "    min_date = df_full['datetime'].min().strftime('%Y-%m-%d %H:%M')\n",
    "    max_date = df_full['datetime'].max().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    print(f\"Available Stations: {stations}\")\n",
    "    print(f\"Data range: {min_date} to {max_date}\")\n",
    "\n",
    "    # Loop for interactive prediction until user enters -1\n",
    "    while True:\n",
    "        try:\n",
    "            # Check if there are any available stations\n",
    "            if not stations:\n",
    "                print(\"Error: No station data available after processing.\")\n",
    "                return\n",
    "\n",
    "            input_station_id_raw = input(f\"Enter Station ID ({stations[0]}-{stations[-1]}) or -1 to exit: \")\n",
    "            input_station_id = int(input_station_id_raw)\n",
    "\n",
    "            if input_station_id == -1:\n",
    "                print(\"Exiting interactive prediction.\")\n",
    "                break\n",
    "\n",
    "            if input_station_id not in stations:\n",
    "                print(\"Invalid Station ID. Please choose from the available list.\")\n",
    "                continue\n",
    "\n",
    "            input_date_str = input(\"Enter Date and Hour (YYYY-MM-DD HH): \")\n",
    "            # Ensure input datetime is timezone-aware (UTC) for correct comparison\n",
    "            input_datetime = pd.to_datetime(input_date_str, format='%Y-%m-%d %H', utc=True)\n",
    "\n",
    "            if input_datetime < pd.to_datetime(min_date, utc=True) or input_datetime > pd.to_datetime(max_date, utc=True):\n",
    "                 print(f\"Date is outside the available range ({min_date} to {max_date}). Please try again.\")\n",
    "                 continue\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input format. Please follow the required format (e.g., 2023-01-15 05).\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "        # --- 2. Extract Current Conditions and Lagged Features ---\n",
    "        # Find the row in the full data matching the user's input\n",
    "        input_row = df_full[\n",
    "            (df_full['station_id'] == input_station_id) &\n",
    "            (df_full['datetime'] == input_datetime)\n",
    "        ]\n",
    "\n",
    "        if input_row.empty:\n",
    "            print(f\"\\nüî¥ Error: No data found for Station {input_station_id} at {input_date_str}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the features needed for prediction\n",
    "        input_data = input_row[CONFIG['FEATURES']].iloc[0]\n",
    "\n",
    "        # --- 3. Display Current Conditions ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"CURRENT CONDITIONS FOR STATION {input_station_id} AT {input_datetime}:\")\n",
    "        print(f\"  Air Temp:       {input_data['air_temp_c']:.2f} ¬∞C\")\n",
    "        print(f\"  Relative Hum:   {input_data['rel_hum_percent']:.1f} %\")\n",
    "        print(f\"  Dew Point:      {input_data['dew_point_c']:.2f} ¬∞C\")\n",
    "        print(f\"  Wind Speed:     {input_data['wind_speed_m_s']:.2f} m/s\")\n",
    "        print(f\"  --- Lagged Temp: {input_data['temp_lag_6']:.2f} ¬∞C (6h ago)\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # --- 4. Prediction ---\n",
    "        print(\"\\nFORECASTED MINIMUM TEMPERATURE & FROST RISK:\")\n",
    "\n",
    "        # XGBoost models require a 2D array/DataFrame input\n",
    "        X_predict = pd.DataFrame([input_data])\n",
    "\n",
    "        for h in CONFIG['horizon']:\n",
    "            if trained_models[h]['clf'] is None or trained_models[h]['reg'] is None:\n",
    "                print(f\"| H={h}h Forecast: No models trained for this horizon due to lack of data.\")\n",
    "                continue\n",
    "\n",
    "            clf = trained_models[h]['clf']\n",
    "            reg = trained_models[h]['reg']\n",
    "\n",
    "            # Predict minimum temperature\n",
    "            min_temp_pred = reg.predict(X_predict)[0]\n",
    "\n",
    "            # Predict frost probability\n",
    "            frost_prob = clf.predict_proba(X_predict)[:, 1][0]\n",
    "            frost_risk = \"HIGH RISK (Frost Likely)\" if frost_prob > 0.25 else \"LOW RISK (No Frost)\"\n",
    "\n",
    "            print(f\"| H={h}h Forecast:\")\n",
    "            print(f\"|   Min Temp: {min_temp_pred:.2f} ¬∞C\")\n",
    "            print(f\"|   Frost Prob: {frost_prob:.2f} -> {frost_risk}\")\n",
    "\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Step 1: Data Preparation ---\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"PHASE 1: DATA LOADING AND PREPROCESSING\")\n",
    "    print(\"#\"*60)\n",
    "    df_full, train_data_by_horizon = load_and_preprocess_data(CIMIS_GZ_FILE_NAME, CIMIS_CSV_FILE_NAME)\n",
    "\n",
    "    # Check for empty dataframes after loading\n",
    "    if df_full.empty:\n",
    "        print(\"Data loading failed. Exiting script.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Step 2: Model Training ---\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"PHASE 2: MODEL TRAINING\")\n",
    "    print(\"#\"*60)\n",
    "    trained_models = train_final_models(train_data_by_horizon)\n",
    "\n",
    "    # --- Step 3: Interactive Prediction ---\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"PHASE 3: STARTING INTERACTIVE PREDICTION\")\n",
    "    print(\"#\"*60)\n",
    "    interactive_prediction(df_full, trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8a9ac-a894-4ea4-975f-0846e249b108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
